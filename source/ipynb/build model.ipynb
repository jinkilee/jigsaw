{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop, Adam, Nadam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "#from keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "#from keras.models import Model, model_from_json, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXWORD = 250\n",
    "BATCHSIZE = 256\n",
    "EMBEDDINGS_DIMENSION = 300\n",
    "NUMWORD = len(pickle.load(open('selected_word.pkl','rb'))) + 1\n",
    "LEARNING_RATE = 0.002\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/jigsaw/train.csv.zip', compression='zip')\n",
    "data.drop(columns='comment_text', inplace=True)\n",
    "data = convert_dataframe_to_bool(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[47611, 24881, 44071, 10130, 24978, 49, 41166,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[47427, 53114, 45, 53446, 47611, 52705, 28553,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[47611, 24881, 45999, 1685, 50441, 12505, 3710...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target  severe_toxicity  obscene  identity_attack  insult  threat  \\\n",
       "0  59848   False              0.0      0.0              0.0     0.0     0.0   \n",
       "1  59849   False              0.0      0.0              0.0     0.0     0.0   \n",
       "2  59852   False              0.0      0.0              0.0     0.0     0.0   \n",
       "\n",
       "   asian  atheist  bisexual  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "0    NaN      NaN       NaN  ...  rejected      0    0    0      0         0   \n",
       "1    NaN      NaN       NaN  ...  rejected      0    0    0      0         0   \n",
       "2    NaN      NaN       NaN  ...  rejected      0    0    0      0         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n",
       "0              0.0                         0                         4   \n",
       "1              0.0                         0                         4   \n",
       "2              0.0                         0                         4   \n",
       "\n",
       "                                          vectorized  \n",
       "0  [47611, 24881, 44071, 10130, 24978, 49, 41166,...  \n",
       "1  [47427, 53114, 45, 53446, 47611, 52705, 28553,...  \n",
       "2  [47611, 24881, 45999, 1685, 50441, 12505, 3710...  \n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized = pickle.load(open('train_vectorized.pkl','rb'))\n",
    "data.loc[:,'vectorized'] = vectorized\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "data = data.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1443899, 45), (360975, 45))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data\n",
    "n_train = int(data.shape[0]*0.8)\n",
    "train = data[:n_train]\n",
    "valid = data[n_train:]\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 70\n",
    "STEPS = int(train.shape[0]/BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(vector, label, batch_size=256):\n",
    "    start = 0\n",
    "    while True:\n",
    "        if start >= len(vector):\n",
    "            start %= batch_size\n",
    "        batch_x = vector[start:start+batch_size]\n",
    "        batch_x = pad_sequences(batch_x, padding='post', maxlen=MAXWORD)\n",
    "        \n",
    "        \n",
    "        _tmp_y = label[start:start+batch_size]\n",
    "        _tmp_y = np.where(_tmp_y, 1, 0)\n",
    "        #batch_y = label[start:start+batch_size]\n",
    "        \n",
    "        batch_y = np.zeros((len(_tmp_y),2))\n",
    "        batch_y[np.arange(len(_tmp_y)), _tmp_y] = 1\n",
    "\n",
    "        start += batch_size\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(train.vectorized.values, train.target.values, batch_size=BATCHSIZE)\n",
    "valid_gen = generator(valid.vectorized.values, valid.target.values, batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model layers.\n",
    "def get_convolutional_neural_net_layers():\n",
    "    \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "    sequence_input = Input(shape=(MAXWORD,), dtype='int32')\n",
    "    embedding_layer = Embedding(NUMWORD,\n",
    "                                EMBEDDINGS_DIMENSION,\n",
    "                                #weights=[embedding_matrix],\n",
    "                                input_length=MAXWORD,\n",
    "                                trainable=False)\n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(5, padding='same')(x)\n",
    "    x = Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(40, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "    return sequence_input, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "# Compile model.\n",
    "print('compiling model')\n",
    "input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=LEARNING_RATE),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h5_filename = 'tmp_model.h5'\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.75, patience=3, min_delta=0.001,\n",
    "                          mode='max', min_lr=1e-5, verbose=1),\n",
    "    ModelCheckpoint(model_h5_filename, monitor='val_top_3_accuracy', mode='max', save_best_only=True,\n",
    "                    save_weights_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1596/5640 [=======>......................] - ETA: 17:19 - loss: 0.2340 - acc: 0.9283"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_gen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    "    validation_data=valid_gen, validation_steps=100,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
